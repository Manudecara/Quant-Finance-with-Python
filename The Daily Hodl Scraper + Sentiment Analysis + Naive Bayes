#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Jan 29 14:34:22 2021

@author: manu
"""

import requests
from requests import get
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from time import sleep
from random import randint
from collections import Counter
from datetime import datetime
from pandas_datareader import data as pdr
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns 
from plotnine import *


#headers = {"Accept-Language": "en-US,en;q=0.5"}
dates = []
titles = []
pages = np.arange(1, 6000)

for page in pages: 
  page = requests.get("https://dailyhodl.com/bitcoin-news/page/" + str(page))
  soup = BeautifulSoup(page.text, 'html.parser')
  scrapes = soup.find_all('article', class_='jeg_post jeg_pl_md_2 format-standard')
  sleep(randint(5,10))

  for scrape in scrapes:
      date = scrape.find('div', class_='jeg_meta_date')
      title = scrape.find('h3', class_='jeg_post_title')
      if None in (date, title):
          continue
      dates.append(date.text.strip())
      titles.append(title.text.strip())

tdh_data = pd.DataFrame({
'titles': titles,
'dates': dates
})

tdh_data.index = tdh_data['dates']
tdh_data.index = pd.to_datetime(tdh_data.index)
tdh_data = tdh_data[["titles"]]
print(tdh_data)
print(tdh_data.dtypes)
print(tdh_data.isnull().sum())

# to move all your scraped data to a CSV file
tdh_data.to_csv('tdh_data.csv')
tdh_data = pd.read_csv('~/Quant Finance/OakTree & Lion/tdh_data.csv')
tdh_data.index = tdh_data['dates']
tdh_data.index = pd.to_datetime(tdh_data.index)
tdh_data = tdh_data[["titles"]]



from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
import pandas_datareader as pdr
import matplotlib.pyplot as plt

#Get Crypto Data
df = pdr.DataReader("BTC-USD",  "yahoo", datetime(2018,1,30), datetime(2021,1,30))
df.head()
df.info()
df = df[~df.index.duplicated()]
df['Volume'] = df['Volume'].astype(float)
df['Volume'] = df['Volume']/1000
df['Open'] = df['Open'].shift(1)
df['High'] = df['High'].shift(1)
df['Low'] = df['Low'].shift(1)
df['Close'] = df['Close'].shift(1)
df['Volume'] = df['Volume'].shift(1)
df = df[['High','Low','Open','Close', 'Volume']]
df = df.dropna()
df.info()
type(df)
type(df.index)


#Apply VADER and add it to dataframe
cs = []
for row in range(len(tdh_data)):
    cs.append(analyzer.polarity_scores(tdh_data['titles'].iloc[row])['compound'])
tdh_data['compound_vader_score'] = cs
tdh_data.head()

#Aggregate by mean
tdh_data = tdh_data.groupby(['dates'])['compound_vader_score'].agg('mean')
tdh_data = pd.DataFrame(tdh_data)


# VADER trade calls - with threshold
vader_Buy=[]
vader_Sell=[]
vader_Hold=[]

for i in range(len(tdh_data)):
    if tdh_data['compound_vader_score'].values[i] > 0.10:
        print("Trade Call for {row} is Buy.")
        vader_Buy.append(tdh_data.index[i])
    elif tdh_data['compound_vader_score'].values[i] < -0.10:
        print("Trade Call for {row} is Sell.")
        vader_Sell.append(tdh_data.index[i])
    elif (tdh_data['compound_vader_score'].values[i] > -0.10) & (tdh_data['compound_vader_score'].values[i] < 0.05):
        print("Trade Call for {row} is Hold.")
        vader_Hold.append(tdh_data.index[i])

vader_buy = []
for i in range(len(df)):
    if df.index[i] in vader_Buy:
        vader_buy.append(i)
        
vader_sell = []
for i in range(len(df)):
    if df.index[i] in vader_Sell:
        vader_sell.append(i)
        
vader_hold = []
for i in range(len(df)):
    if df.index[i] in vader_Hold:
        vader_hold.append(i)
        
     
#Plot of Sentiment Strategy
plt.figure(figsize=(20, 10),dpi=80)
plt.plot(df.index, df['Close'],'-^', markevery=vader_buy, ms=15, color='green')
plt.plot(df.index, df['Close'],'-v', markevery=vader_sell, ms=15, color='red')
plt.plot(df.index, df['Close'])
plt.xlabel('Date',fontsize=14)
plt.ylabel('Price in Dollars', fontsize = 14)
plt.xticks(rotation='60',fontsize=12)
plt.yticks(fontsize=12)
plt.title('Trade Calls - VADER', fontsize = 16)
plt.legend(['Buy','Sell','Close'])
plt.grid()
plt.show() 

tdh_data.info()
tdh_data['compound_vader_score'] = tdh_data['compound_vader_score'].astype(float)
tdh_data["labels"] = 'Neutral Sentiment'
tdh_data['labels'] = tdh_data['labels'].astype(str)

#Fill missing dates
tdh_data = tdh_data.reindex(df.index, fill_value = 0)
print(tdh_data)
tdh_data["labels"][(tdh_data['compound_vader_score'] > 0.10)] = 'Positive Sentiment'
tdh_data["labels"][(tdh_data['compound_vader_score'] < -0.10)] = 'Negative Sentiment'
tdh_data["labels"].value_counts()
tdh_data['labels'][(tdh_data['labels'] == 0)] = "Neutral Sentiment"


#Apply Machine Learning

#General Sklearn
from sklearn.model_selection import train_test_split
import sklearn.model_selection
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer

#Supervised Learning
#Naive Bayes
from sklearn.naive_bayes import GaussianNB


#Split dataset into 6(3/3) for ML appications
train_ratio = 0.75
validation_ratio = 0.15
test_ratio = 0.10
X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 0:5], tdh_data['labels'], test_size = 1 - train_ratio, shuffle = False)
X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = test_ratio/(test_ratio + validation_ratio), shuffle = False) 

#Check for state inclusion
y_train.value_counts()
y_val.value_counts()
y_test.value_counts()

nb = GaussianNB()
model = nb.fit(X_train, y_train)
y_pred = model.predict(X_val)

#Check scores on Validation dataset
accuracy_val = accuracy_score(y_val, y_pred)
print ('Validation_data Accuracy: %.2f' %accuracy_val)
confusion_matrix(y_val, y_pred)

# Combine Train + Validation data to use as 1 set of training data
X_train = [X_train, X_val]
X_train = pd.concat(X_train)
X_train.head()
y_train = [y_train, y_val]
y_train = pd.concat(y_train)
y_train.head()

final_model = nb.fit(X_train, y_train)
y_pred = final_model.predict(X_test)

#Check scores on Testing dataset
accuracy_test = accuracy_score(y_test, y_pred)
print ('Test_data Accuracy: %.2f' %accuracy_test)
confusion_matrix(y_test, y_pred)

#Plot of true and predicted sentiments on testing data
X_test['index'] = X_test.index
y_test = pd.DataFrame(y_test)
ggplot() + geom_line(aes(x= X_test['index'], y= X_test['Close'], colour = y_test['labels'])) + geom_line(aes(x= X_test['index'], y= X_test['Close']*2, colour = y_pred))



